{
  "summary": {},
  "version": "2.0.0",
  "results": [
    {
      "version": 1,
      "platform": "GCP",
      "category": "Logging and Monitoring",
      "resource": "GKE",
      "title": "Ensure Stackdriver Logging and Monitoring is configured",
      "description": "Exporting logs and metrics to a dedicated, persistent datastore such as Stackdriver ensures availability of audit data following a cluster security event, and provides a central location for analysis of log and metric data collated from multiple sources.",
      "remediation": "Ensure `--enable-stackdriver-kubernetes` is set during cluster configuration, or run `gcloud container clusters update` and pass the `--enable-stackdriver-kubernetes` flag to enable the addons.",
      "validation": "Run `gcloud container clusters describe` and review the configuration under `loggingService` and `monitoringService`.  They should be configured with `logging.googleapis.com/kubernetes` and `monitoring.googleapis.com/kubernetes`, respectively.",
      "severity": 0.8,
      "effort": 0.2,
      "references": [
        {
          "url": "https://cloud.google.com/monitoring/kubernetes-engine",
          "ref": "GKE Monitoring and Logging"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Identity and Access Management",
      "resource": "GKE",
      "title": "Ensure Basic Authentication is disabled",
      "description": "Until GKE 1.12, Basic Authentication was enabled by default on all clusters unless explicitly disabled.  Clusters that were created at or before version 1.12 and have been upgraded since will still have a valid username and password credential that grants full cluster access.  These credentials cannot be revoked or rotated without recreating the cluster.  Furthermore, they are available in clear-text via the `gcloud container clusters list/get` command, and many IAM Roles contain the `container.clusters.get` and `container.clusters.list` permissions, including `Project Viewer`.  When coupled with network access to the GKE API server, a clear path to become `cluster-admin` is possible.",
      "remediation": "Recreate the GKE cluster from a recent version (1.12+) ensuring the `--no-enable-basic-auth` flag set or supply a <blank> value for the `master_auth.username` field when using Terraform.",
      "validation": "Run `gcloud container clusters get` and review the `masterAuth` configuration block.  There should not be a `username` and `password` field with values.",
      "severity": 1,
      "effort": 0.9,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/iam-integration",
          "ref": "Auth"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Network Access Control",
      "resource": "GKE",
      "title": "Ensure GKE Nodes are not public",
      "description": "By default, GKE nodes are created with both private and public IP addresses assigned, and the default Firewall Rules in the default Network permit remote access via SSH on TCP/22 from `0.0.0.0/0`.  Preventing GKE Nodes from being assigned a public IP address ensures that remote access attempts using SSH cannot be routed from the public Internet.  Should a pod become compromised and escape to the underlying node, it's possible to use that access to add SSH credentials to the host configuration.  However, that node will not be directly accessible from the Internet for SSH access if a public IP is not assigned.",
      "remediation": "Recreate the GKE cluster ensuring the `--enable-private-nodes` flag is configured.  Ensure administrators have another mechanism such as a Bastion Host in the same VPC or Cloud Identity-Aware Proxy access is available if SSH access is still required.",
      "validation": "Run `gcloud container clusters get` and review the `privateClusterConfig` configuration block. Ensure `enablePrivateNodes` is set to `true`.",
      "severity": 0.5,
      "effort": 0.9,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters",
          "ref": "GKE Private Nodes"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Network Access Control",
      "resource": "GKE",
      "title": "Ensure the GKE Control Plane is not public",
      "description": "By default, the GKE Control Plane (Kubernetes API) is assigned a public IP address and the network access control allows access from `0.0.0.0/0`.  When a new vulnerability is found in the Kubernetes API server, the scope of potential attackers is the entire Internet.  By configuring the GKE Cluster with a private IP or by adding a restricted list of CIDRs with access to the API server, the scope is greatly limited and can buy valuable time to patch/upgrade.  Also, if credentials from in-cluster service accounts and Kubernetes components are leaked, they cannot be leveraged against the API server from any location.",
      "remediation": "Recreate the GKE Cluster with the `--enable-private-endpoint` flag set.  If the cluster cannot be recreated with only a private IP, ensure that `--master-authorized-networks` is configured with a limited set of CIDR ranges.'",
      "validation": "Run `gcloud container clusters get` and review the `privateClusterConfig` configuration block. Ensure `enablePrivateEndpoint` is set to `true`.  Or, ensure the `masterAuthorizedNetworksConfig` configuration block has `cidrBlocks` that do not include `0.0.0.0/0`.",
      "severity": 1,
      "effort": 0.2,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters",
          "ref": "GKE Private Control Plane"
        },
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks",
          "ref": "GKE Master Authorized Networks"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Network Access Control",
      "resource": "GKE",
      "title": "Ensure the GKE Cluster has the Network Policy managed addon enabled",
      "description": "By default, all Kubernetes pods inside a cluster can communicate with each other--even across namespaces.  All production Kubernetes clusters should have support enabled for being able to define Layer 4 `NetworkPolicy` resources, and in many cases, this is an optional addon that must be explicitly enabled.  With this support enabled, it's possible to define policies inside the cluster that restrict inbound and outbound network traffic to pods within namespaces and provide micro-segmentation.  Should a pod become compromised, strict `NetworkPolicy` configurations can significantly limit the attacker's ability to move laterally via the network.",
      "remediation": "During cluster creation, ensure the `--enable-network-policy` flag is configured.  For existing clusters, run `gcloud container clusters update cluster-name --update-addons=NetworkPolicy=ENABLED` followed by `gcloud container clusters update cluster-name --enable-network-policy`.  Note that this forces all nodepools to be recreated to have the CNI changes take effect.",
      "validation": "Run `gcloud container clusters get` and review the `networkPolicy` configuration block. Ensure `provider` is set to `CALICO` and `enabled` is `true`.",
      "severity": 0.9,
      "effort": 0.5,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy#enabling_network_policy_enforcement",
          "ref": "GKE Network Policy"
        },
        {
          "url": "https://kubernetes.io/docs/concepts/services-networking/network-policies/#the-networkpolicy-resource",
          "ref": "Network Policy"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Identity and Access Management",
      "resource": "GKE",
      "title": "Ensure GKE Cluster Nodepools are created with minimal OAuth Access Scopes and dedicated Service Accounts",
      "description": "By default, GKE Cluster Nodepools are assigned the default Compute service account in the project, and this service account is bound to the `Project Editor` IAM Role which has wide-ranging permissions in the project across nearly all services.  However, these service account credentials bound to the GCE nodes that make up the GKE Nodepool can be further restricted by setting service-specific OAuth Scopes.  Unless additional network restrictions are place on pods running inside the cluster, this means that any pod in any namespace can obtain access to these instance credentials via the GCP Metadata API (169.254.169.254).  Before GKE 1.12, the OAuth Scopes commonly contained `compute` or even `cloud-platform`.  When combined with the `Project Editor` IAM Role, these instance credentials allow near full access to all `gcloud compute` commands or all gcloud services, respectively.  Since GKE 1.12, the scopes needed for proper management function are now a fixed list.  Pods wanting to gain access to credentials for accessing GCP APIs should use the Workload Identity feature to both block access to the instance credentials via the Metadata API and to map GCP Service Accounts to Kubernetes Service Accounts.",
      "remediation": "Create a dedicated GCP Service Account.  Create and bind an IAM Role with `roles/monitoring.metricWriter`, `monitoring.viewer`, and `logging.logWriter` permissions to the dedicated GCP Service Account.  Specify that service account during Nodepool creation via the `--service-account` flag.  Recreation is necessary for existing nodepools.",
      "validation": "Run `gcloud container clusters get` and review the `nodeConfig` configuration block. Ensure `serviceAccount` is not set to `default` and `oauthScopes` contains only `https://www.googleapis.com/auth/devstorage.read_only`, `logging.write`, `monitoring`, `service.management.readonly`, `servicecontrol`, and `trace.append`.",
      "severity": 0.7,
      "effort": 0.2,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/access-scopes",
          "ref": "GKE OAuth Access Scopes"
        },
        {
          "url": "https://cloud.google.com/compute/docs/access/service-accounts#service_account_permissions",
          "ref": "GCP Service Account Permissions"
        },
        {
          "url": "https://cloud.google.com/compute/docs/access/service-accounts#default_service_account",
          "ref": "GCP Default Service Account"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good/default-pool"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Host and Cluster Security",
      "resource": "GKE",
      "title": "GKE Node Pools should use the COS or COS_CONTAINERD Operating System",
      "description": "GKE Nodes can leverage either Container-Optimized OS or Ubuntu-based operating system images.  Unless there is a very specific use-case that a Container-Optimized OS image cannot support such as installed certain drivers and/or kernel modules, Ubuntu nodes are not recommended.  Container-Optimized OS is a fully hardened operating system designed specifically to run containerized workloads with a high degree of security, and it receives automatic updates from Google.  The track record for security issues that affect Ubuntu nodes in GKE that did not affect COS nodes is also important to consider.",
      "remediation": "Configure your GKE Node Pools to leverage either the COS or COS_CONTAINERD image type.  The COS image leverages Docker, and the COS_CONTAINERD image implements only containerd and does not use the commonly known Docker socket at `/var/run/docker.sock` which allows applications that can access that socket to effectively be `root` on the host.  If your workloads do not require the ability to mount the docker socket for activities such as image building in-cluster or certain security features, COS_CONTAINERD offers an even smaller attack surface than COS.  Considerations: changing the image type recreates the nodes in the node pool.",
      "validation": "Run `gcloud container clusters describe <clustername> --format=json | jq -r 'select(.nodePools[].config.imageType | test(\"^COS\")) | .name'` and ensure that the cluster's name is listed.",
      "severity": 0.5,
      "effort": 0.9,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/node-images",
          "ref": "GKE Node Images"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good/default-pool"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Host and Cluster Security",
      "resource": "GKE",
      "title": "GKE Workload Identity should be enabled and enforcing metadata protection on all NodePools",
      "description": "Currently, all pods have the ability to reach the Instance Metadata API corresponding to the underlying node.  By extension, those pods can access the APIs and data used to bootstrap the Kubernetes worker node.  The credentials used to bootstrap a Kubernetes worker node are very commonly sufficient to be used to privilege escalate to `cluster-admin`.  Also by extension, this means that every container image ever run in this cluster in the non-production namespace has had the ability to reach and export these credentials.  Therefore, it's very important for a cluster's security posture to prevent pods from being able to reach the Instance Metadata API to fetch those bootstrapping credentials.",
      "remediation": "Configure Workload Identity on the cluster and every node pool in the cluster with the GKE_METADATA setting enabled.  Alternatively, deploy an egress NetworkPolicy blocking egress to 169.254.169.254 for all non-kube-system namespaces.",
      "validation": "Run `gcloud container clusters describe <clustername> --format=json | jq -r 'select(.workloadIdentityConfig.workloadPool | test(\"svc.id.goog\")) | .name'` and ensure that the cluster's name is listed.",
      "severity": 0.9,
      "effort": 0.5,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity",
          "ref": "GKE Workload Identity"
        },
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#workload_identity",
          "ref": "Hardening GKE"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good"
        },
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good/default-pool"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 2,
        "total": 2
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Operations and Response",
      "resource": "GKE",
      "title": "Production GKE Clusters should have a highly-available control plane",
      "description": "By default, GKE creates a `zonal` cluster.  That is, a cluster where the single control plane GCE instance is deployed in one GCP availability zone.  GKE clusters can also be configured as `regional` clusters in which three control plane GCE instances can be deployed evenly across three availability zones at no direct, additional cost.  Having three control plane instances insulates from a single control plane instance failure and allows for zero-downtime API server upgrades.",
      "remediation": "For all production GKE clusters, configure the `location` as the region name instead of the zone name.  This requires rebuilding the cluster if it is already deployed as a zonal cluster.",
      "validation": "Run `gcloud container clusters describe <clustername> --format=json | jq -r 'select(.location | test(\"^[a-z]+-[a-z0-9]+$\")) | .name'` and ensure that the cluster's name is listed.",
      "severity": 0.9,
      "effort": 0.9,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/regional-clusters",
          "ref": "GKE Regional Clusters"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "GCP",
      "category": "Host and Cluster Security",
      "resource": "GKE",
      "title": "GKE Shielded Nodes should be enabled on all NodePools",
      "description": "Starting in GKE 1.13.6 and later, GKE Worker nodes can be provisioned with a Virtual Trusted Platform Module (vTPM) that can be used to cryptographically verify the integrity of the boot process and to securely distribute the bootstrapping credentials used by the Kubelet to attach the node to the cluster on first boot.  Without this feature, the Kubelet's bootstrapping credentials are available via the GCE Metadata API, and that can be accessed by any Pod unless additional protections are put in place.  These credentials can be leveraged to escalate to cluster-admin in most situations.",
      "remediation": "Modify the cluster node pool configuration to enable shielded nodes (--enable-shielded-nodes) and secure boot (--shielded-secure-boot).  This will remove the sensitive bootstrapping credentials from the GCE Metadata API and enable additional verification checks to ensure the worker nodes have not been compromised at a fundamental level.  Considerations: The nodes must be running the COS or COS_CONTAINERD operating system, and enabling this change will require a node pool rolling redeployment performed at the next maintenance window.",
      "validation": "Run `gcloud container clusters describe <clustername> --format=json | jq -r 'select(.nodePools[].config.shieldedInstanceConfig.enableIntegrityMonitoring==true and .nodePools[].config.shieldedInstanceConfig.enableSecureBoot==true) | \"(.name)\"'` and ensure that the cluster's name is listed.",
      "severity": 0.5,
      "effort": 0.5,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes",
          "ref": "GKE Shielded Nodes"
        },
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#shielded_nodes",
          "ref": "Hardening GKE"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good"
        },
        {
          "status": "passed",
          "resource": "demo1-good/us-central1/gke-good/default-pool"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 2,
        "total": 2
      }
    },
    {
      "version": 1,
      "platform": "K8S",
      "category": "Workload Isolation",
      "resource": "Pods",
      "title": "Ensure pods only run in dedicated namespaces",
      "description": "By default, user-managed resources will be placed in the `default` namespace.  This makes it difficult to properly define policies for RBAC permissions, service account usage, network policies, and more.  Creating dedicated namespaces and running workloads and supporting resources in each helps support proper API server permissions separation and network microsegmentation.",
      "remediation": "Create dedicated namespaces for each type of related workload, and migrate those resources into those namespaces.  Ensure that RBAC permissions are not granted at the cluster scope but per namespace for the application owners at each namespace level.",
      "validation": "Run `kubectl get all` in the `default`, `kube-public`, and if present, `kube-node-lease` namespaces.  There should only be the `kubernetes` service.",
      "severity": 0.2,
      "effort": 0.3,
      "references": [
        {
          "url": "https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/",
          "ref": "Kubernetes Namespaces"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "default namespace"
        },
        {
          "status": "passed",
          "resource": "kube-public namespace"
        },
        {
          "status": "passed",
          "resource": "kube-node-lease namespace"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 3,
        "total": 3
      }
    },
    {
      "version": 1,
      "platform": "K8S",
      "category": "Management and Governance",
      "resource": "Pods",
      "title": "Ensure the Kubernetes Dashboard is not present",
      "description": "While the Kubernetes dashboard is not inherently insecure on its own, it is often coupled with a misconfiguration of RBAC permissions that can unintentionally overgrant access and is not commonly protected with `NetworkPolicies` preventing all pods from being able to reach it.  In increasingly rare circumstances, the Kubernetes dashboard is exposed publicly to the Internet.",
      "remediation": "Instead of running a workload inside the cluster to display a UI, leverage the cloud provider's UI for listing/managing workloads or consider a tool such as Octant running on local systems.  Run `kubectl get pods --all-namespaces -l k8s-app=kubernetes-dashboard` to find pods part of deployments and use kubectl to delete those deployments.",
      "validation": "Running `kubectl get pods --all-namespaces -l k8s-app=kubernetes-dashboard` should not return any pods.",
      "severity": 0.3,
      "effort": 0.1,
      "references": [
        {
          "url": "https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/",
          "ref": "Kubernetes Dashboard"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "Dashboard"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "K8S",
      "category": "Addon Security",
      "resource": "Pods",
      "title": "Ensure Tiller (Helm v2) is not deployed",
      "description": "Helm version 1.x and 2.x rely on an in-cluster deployment named `Tiller` to handle lifecycle management of Kubernetes application bundles called `charts`.  The `Tiller` deployment is commonly granted elevated privileges to be able to carry out creation/deletion of resources contained inside `charts`, and it exposes a gRPC port on TCP/44134 without authentication or authorization, by default.  This combination was common, and it afforded a simple and direct path to escalation to cluster-admin from any pod in the cluster.  Now that Helm v3 no longer relies on an in-cluster component, `Tiller` is a signal that the cluster administrators have not upgraded to the more secure version.",
      "remediation": "Refer to https://helm.sh/docs/topics/v2_v3_migration/ for guidance on migrating away from `Tiller`.  For new cluster deployments, use Helm v3 and above going forward.",
      "validation": "Run `kubectl get pods --all-namespaces -o name | grep tiller` and validate that no pods starting with the name `tiller-deploy-****` exist.",
      "severity": 1,
      "effort": 0.2,
      "references": [
        {
          "url": "https://helm.sh",
          "ref": "Helm"
        },
        {
          "url": "https://helm.sh/docs/faq/#removal-of-tiller",
          "ref": "Tiller v2"
        },
        {
          "url": "https://helm.sh/docs/topics/v2_v3_migration/",
          "ref": "Helm Migration from v2 to v3"
        },
        {
          "url": "https://engineering.bitnami.com/articles/helm-security.html",
          "ref": "Misusing Tiller"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "default/tiller-deploy"
        },
        {
          "status": "passed",
          "resource": "kube-node-lease/tiller-deploy"
        },
        {
          "status": "passed",
          "resource": "kube-public/tiller-deploy"
        },
        {
          "status": "passed",
          "resource": "kube-system/tiller-deploy"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 4,
        "total": 4
      }
    },
    {
      "version": 1,
      "platform": "K8S",
      "category": "Management and Governance",
      "resource": "Pods",
      "title": "Ensure all containers refer to a specific version tag not named latest",
      "description": "When referring to a container image stored in a registry, it's common practice for the owner of the image to tag the most recent image with a semver tag and also the `latest` tag when uploading it.  This is a convenenience for users wanting to work with the most up-to-date image, but it presents an opportunity for inconsistencies inside Kubernetes.  If a deployment with more than one replica references an image with the tag `latest`, the underlying node will pull and run that image at that time.  If the image in the registry is updated with a new `latest` image and the deployment scales the number of replicas such that a new worker node is to run it, that node will potentially pull the newer `latest` image.",
      "remediation": "Review all deployments and pod specifications, and modify any that reference the `latest` tag to use a specific version tag or even the `sha256` hash.  Consider enforcing this practice early with a validation step in the CI/CD pipeline and enforcing the policy with OPA/Gatekeeper or other policy-based admission controller inside the cluster.",
      "validation": "Run `kubectl get po -A -ojsonpath='{..image}' | kubectl get pods --all-namespaces -o jsonpath='{..image}' |tr -s '[[:space:]]' '\n' | sort | uniq -c | grep latest` and ensure no images reference the `latest` tag.",
      "severity": 0.5,
      "effort": 0.2,
      "references": [
        {
          "url": "https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/",
          "ref": "Kubectl List Images"
        },
        {
          "url": "https://kubernetes.io/docs/concepts/configuration/overview/#container-images",
          "ref": "Kubernetes Configuration Best Practices"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "kube-system/calico-node-cgmrm"
        },
        {
          "status": "passed",
          "resource": "kube-system/calico-node-ksnxl"
        },
        {
          "status": "passed",
          "resource": "kube-system/calico-node-sk52q"
        },
        {
          "status": "passed",
          "resource": "kube-system/calico-node-vertical-autoscaler-b889c775f-4p5lp"
        },
        {
          "status": "passed",
          "resource": "kube-system/calico-typha-79799497d7-d45tb"
        },
        {
          "status": "passed",
          "resource": "kube-system/calico-typha-horizontal-autoscaler-d777c75b4-t6wrs"
        },
        {
          "status": "passed",
          "resource": "kube-system/calico-typha-vertical-autoscaler-d9b7979f8-4dn4r"
        },
        {
          "status": "passed",
          "resource": "kube-system/event-exporter-v0.2.5-599d65f456-r84sb"
        },
        {
          "status": "passed",
          "resource": "kube-system/fluentd-gcp-scaler-bfd6cf8dd-7v9k8"
        },
        {
          "status": "passed",
          "resource": "kube-system/fluentd-gcp-v3.1.1-b7zxm"
        },
        {
          "status": "passed",
          "resource": "kube-system/fluentd-gcp-v3.1.1-d48wf"
        },
        {
          "status": "passed",
          "resource": "kube-system/fluentd-gcp-v3.1.1-l4p9s"
        },
        {
          "status": "passed",
          "resource": "kube-system/gke-metadata-server-gd229"
        },
        {
          "status": "passed",
          "resource": "kube-system/gke-metadata-server-pxsg8"
        },
        {
          "status": "passed",
          "resource": "kube-system/gke-metadata-server-z82hz"
        },
        {
          "status": "passed",
          "resource": "kube-system/heapster-gke-5d4687d6b-fbmbw"
        },
        {
          "status": "passed",
          "resource": "kube-system/ip-masq-agent-5f6dk"
        },
        {
          "status": "passed",
          "resource": "kube-system/ip-masq-agent-ndmng"
        },
        {
          "status": "passed",
          "resource": "kube-system/ip-masq-agent-t68rs"
        },
        {
          "status": "passed",
          "resource": "kube-system/kube-dns-5995c95f64-65whd"
        },
        {
          "status": "passed",
          "resource": "kube-system/kube-dns-5995c95f64-qh96l"
        },
        {
          "status": "passed",
          "resource": "kube-system/kube-dns-autoscaler-8687c64fc-zjxw7"
        },
        {
          "status": "passed",
          "resource": "kube-system/kube-proxy-gke-gke-good-default-pool-49d0d21c-qvqw"
        },
        {
          "status": "passed",
          "resource": "kube-system/kube-proxy-gke-gke-good-default-pool-587d7233-7c59"
        },
        {
          "status": "passed",
          "resource": "kube-system/kube-proxy-gke-gke-good-default-pool-b79b736e-zvth"
        },
        {
          "status": "passed",
          "resource": "kube-system/l7-default-backend-8f479dd9-x8fcj"
        },
        {
          "status": "passed",
          "resource": "kube-system/metrics-server-v0.3.1-5c6fbf777-nmsqc"
        },
        {
          "status": "passed",
          "resource": "kube-system/netd-n4b4x"
        },
        {
          "status": "passed",
          "resource": "kube-system/netd-pq7cf"
        },
        {
          "status": "passed",
          "resource": "kube-system/netd-tgmnc"
        },
        {
          "status": "passed",
          "resource": "kube-system/prometheus-to-sd-l2dw2"
        },
        {
          "status": "passed",
          "resource": "kube-system/prometheus-to-sd-ttbhf"
        },
        {
          "status": "passed",
          "resource": "kube-system/prometheus-to-sd-ww56d"
        },
        {
          "status": "passed",
          "resource": "kube-system/stackdriver-metadata-agent-cluster-level-658bd899cd-x9zxx"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 34,
        "total": 34
      }
    },
    {
      "version": 1,
      "platform": "K8S",
      "category": "Management and Governance",
      "resource": "Pods",
      "title": "Ensure all pods reference container images from known sources",
      "description": "By default, Kubernetes allows users with the ability to create pods to reference any container image path, including public registries like DockerHub.  This allows developers to share and use pre-made container images easily, but it enables unvalidated and untrusted code to run inside your cluster with potential access to mounted secrets and service account tokens.  Container images should be verified to be conformant to security standards before being run, and the first step to this is to validate that all container images are being pulled from a known set of registries.  This helps development teams and security teams work from the same base location for running and validating images.",
      "remediation": "Review all deployments and pod specifications, and find any that reference non-approved container registries.  Create a dedicated container registry in your environment, validate those container images meet your security policies, and store/mirror them to that dedicated container registry/registries.  Consider enforcing image sources early with a validation step in the CI/CD pipeline and enforcing the policy with OPA/Gatekeeper or other policy-based admission controller inside the cluster.",
      "validation": "Run `kubectl get po -A -ojsonpath='{..image}' | kubectl get pods --all-namespaces -o jsonpath='{..image}' |tr -s '[[:space:]]' '\n' | sort | uniq -c ` and ensure all images are sourced from the official Kubernetes or cloud provider registries and your own internal container registries.",
      "severity": 0.8,
      "effort": 0.5,
      "references": [
        {
          "url": "https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/",
          "ref": "Kubectl List Images"
        }
      ],
      "resources": [],
      "result": {
        "status": "passed",
        "passed": 0,
        "total": 0
      }
    },
    {
      "version": 1,
      "platform": "K8S",
      "category": "Network Access Control",
      "resource": "Daemonsets",
      "title": "Validate NetworkPolicy-aware enforcement is installed",
      "description": "In GKE and EKS, the supported agent that can implement enforcement of `NetworkPolicy` resources is `Calico`, by Tigera.  In AKS, either `Calico` or AKS' own `azure` addon can implement micro-segmentation.  All of them are not enabled/installed by default and require explicit configuration.  In addition, the Kubernetes API will store `NetworkPolicy` resources, but without enforcement agents running, those never get applied on the nodes and pods which might cause a false sense of security.",
      "remediation": "In GKE, enable Network Policy addon support.  In EKS, install the correct version of Calico for your version of EKS.  For AKS, configure either the `azure` or `calico` network policy addon.",
      "validation": "Run `kubectl get daemonsets -n kube-system` and look for either `calico-node` or `azure-cni-networkmonitor` to be present.",
      "severity": 0.8,
      "effort": 0.5,
      "references": [
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy#enabling_network_policy_enforcement",
          "ref": "GKE Network Policy"
        },
        {
          "url": "https://docs.aws.amazon.com/eks/latest/userguide/calico.html",
          "ref": "EKS Network Policy"
        },
        {
          "url": "https://docs.microsoft.com/en-us/azure/aks/use-network-policies",
          "ref": "AKS Network Policy"
        }
      ],
      "resources": [
        {
          "status": "passed",
          "resource": "NetworkPolicy Daemonset"
        }
      ],
      "result": {
        "status": "passed",
        "passed": 1,
        "total": 1
      }
    },
    {
      "version": 1,
      "platform": "K8S",
      "category": "Network Access Control",
      "resource": "NetworkPolicies",
      "title": "Validate NetworkPolicies are defined in each namespace",
      "description": "While support for `NetworkPolicies` is required in each cluster, the default policy allows all ingress and egress traffic to each pod.  Each namespace should have one or more `NetworkPolicy` resources defined to explicitly grant all ingress and egress access and to deny all other traffic.  Proper network access control at the pod level significantly reduces the ability for an attacker who has compromised a pod to move laterally to attack other pods or externally to instance metadata or cloud APIs.",
      "remediation": "Deploy one or more `NetworkPolicy` resources in each namespace.  The most secure approach is a `default-deny-all` policy that blocks all ingress and egress traffic for that namespace followed by individual policies that allow the explicit traffic necessary.",
      "validation": "Run `kubectl get networkpolicies --all-namespaces` and ensure each namespace has the desired policies defined.",
      "severity": 0.8,
      "effort": 0.5,
      "references": [
        {
          "url": "https://kubernetes.io/docs/concepts/services-networking/network-policies/",
          "ref": "Kubernetes Network Policies"
        },
        {
          "url": "https://github.com/ahmetb/kubernetes-network-policy-recipes",
          "ref": "Kubernetes Example Network Policies"
        }
      ],
      "resources": [
        {
          "status": "failed",
          "resource": "default"
        },
        {
          "status": "failed",
          "resource": "kube-system"
        }
      ],
      "result": {
        "status": "failed",
        "passed": 0,
        "total": 2
      }
    },
    {
      "version": 1,
      "platform": "K8S",
      "category": "Workload Isolation",
      "resource": "Pods",
      "title": "Ensure resource specification enforcement is installed",
      "description": "Kubernetes RBAC determines who can create/read/update/delete resources.  However, users with the RBAC permission to create pods, for example, can define a pod specification that allows for direct access to the underlying worker nodes.  This can and has led to privilege escalation by attacking other workloads after escaping their container.  Natively, the `PodSecurityPolicy` admission controller allows administrators to define policies for pod specifications to prevent them from running as root, accessing the node's filesystem, running as a `privileged` container, and more.  Effectively configured `PodSecurityPolicies` can greatly reduce the negative effects of a malicious workload, but it is limited to just resources that define `pod` specifications and `templates`.  Solutions like OPA/Gatekeeper and K-rail leverage the `ValidatingWebhookConfiguration` resource in the API server to allow external applications the ability to apply custom logic to a given request to create/update a resource and allow or deny that request.  These deployments can validate the configuration of any resource (including pod specifications) with the appropriate policies in place.",
      "remediation": "For basic needs limited to pod specification enforcement, consider enabling the `PodSecurityPolicy` admission controller and defining policies that do not allow privileged settings.  For intermediate to advanced use cases, consider deploying OPA/Gatekeeper or K-rail inside the cluster and define policies that enforce similar constraints on `pods` as well as other resources as needed.",
      "validation": "Run `kubectl get psps --all-namespaces` to identify `PodSecurityPolicy` resources in place, or run `kubectl get deployments --all-namespaces` and look for `gatekeeper` or `k-rail` deployments to be present.",
      "severity": 0.9,
      "effort": 0.7,
      "references": [
        {
          "url": "https://github.com/open-policy-agent/gatekeeper",
          "ref": "OPA/Gatekeeper"
        },
        {
          "url": "https://github.com/cruise-automation/k-rail",
          "ref": "K-rail"
        },
        {
          "url": "https://kubernetes.io/docs/concepts/policy/pod-security-policy/",
          "ref": "Kubernetes PodSecurityPolicy"
        },
        {
          "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies",
          "ref": "GKE PodSecurityPolicy"
        },
        {
          "url": "https://docs.aws.amazon.com/eks/latest/userguide/pod-security-policy.html",
          "ref": "GKE PodSecurityPolicy"
        },
        {
          "url": "https://docs.microsoft.com/en-us/azure/aks/use-pod-security-policies",
          "ref": "AKS PodSecurityPolicy"
        }
      ],
      "resources": [
        {
          "status": "failed",
          "resource": "OPA/Gatekeeper"
        },
        {
          "status": "failed",
          "resource": "K-rail"
        },
        {
          "status": "failed",
          "resource": "PodSecurityPolicy"
        }
      ],
      "result": {
        "status": "failed",
        "passed": 0,
        "total": 3
      }
    }
  ]
}
